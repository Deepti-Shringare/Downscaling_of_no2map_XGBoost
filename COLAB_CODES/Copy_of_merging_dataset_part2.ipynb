{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepti-Shringare/Downscaling_of_no2map_XGBoost/blob/main/COLAB_CODES/Copy_of_merging_dataset_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKvpWdvLxQVB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import ast\n",
        "import os\n",
        "from shapely.geometry import Point\n",
        "from google.colab import drive\n",
        "\n",
        "# ============= MOUNT GOOGLE DRIVE =============\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --------------------\n",
        "# STEP 1: Load CPCB daily groundtruth\n",
        "# --------------------\n",
        "cpcb = pd.read_csv(\"/content/drive/MyDrive/DELHI/cpcb_daily.csv\")\n",
        "cpcb[\"date\"] = pd.to_datetime(cpcb[\"date\"])\n",
        "cpcb[\"month\"] = cpcb[\"date\"].dt.to_period(\"M\")  # YYYY-MM\n",
        "cpcb_monthly = cpcb.groupby([\"Station Name\",\"month\"])[\"NO2\"].mean().reset_index()\n",
        "\n",
        "# --------------------\n",
        "# STEP 2: Load station locations\n",
        "# --------------------\n",
        "stations = pd.read_csv(\"/content/drive/MyDrive/DELHI/delhi_station_population_2024.csv\")\n",
        "gdf_stations = gpd.GeoDataFrame(\n",
        "    stations,\n",
        "    geometry=gpd.points_from_xy(stations[\"Longitude\"], stations[\"Latitude\"]),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "# --------------------\n",
        "# STEP 3: Function to parse one satellite file\n",
        "# --------------------\n",
        "def parse_satellite_file(file_path, month_str):\n",
        "    sat = pd.read_csv(file_path)\n",
        "\n",
        "    # Parse lat/lon from geo column\n",
        "    def parse_coords(geo_str):\n",
        "        try:\n",
        "            geo_dict = ast.literal_eval(geo_str)\n",
        "            return geo_dict[\"coordinates\"][0], geo_dict[\"coordinates\"][1]\n",
        "        except:\n",
        "            return None, None\n",
        "\n",
        "    sat[[\"lon\", \"lat\"]] = sat[\".geo\"].apply(lambda g: pd.Series(parse_coords(g)))\n",
        "\n",
        "    gdf_sat = gpd.GeoDataFrame(\n",
        "        sat,\n",
        "        geometry=gpd.points_from_xy(sat[\"lon\"], sat[\"lat\"]),\n",
        "        crs=\"EPSG:4326\"\n",
        "    )\n",
        "\n",
        "    # nearest join\n",
        "    joined = gpd.sjoin_nearest(gdf_stations, gdf_sat, how=\"left\")\n",
        "\n",
        "    # assign month\n",
        "    joined[\"month\"] = month_str\n",
        "\n",
        "    return joined[[\"STATION NAME\", \"month\", \"tropospheric_NO2_column_number_density\"]]\n",
        "\n",
        "# --------------------\n",
        "# STEP 4: Loop over all satellite monthly files\n",
        "# --------------------\n",
        "sat_dir = \"/content/drive/MyDrive/DELHI/\" # Corrected to directory path\n",
        "all_sat = []\n",
        "\n",
        "for file in os.listdir(sat_dir):\n",
        "    if file.endswith(\".csv\") and \"TROPOMI_NO2_Delhi\" in file: # Added check for filename pattern\n",
        "        # Extract month from filename\n",
        "        # Example: \"TROPOMI_NO2_Delhi_April2024.csv\"\n",
        "        month_name = file.split(\"_\")[-1].replace(\".csv\",\"\")  # e.g. \"April2024\"\n",
        "        month_str = pd.to_datetime(month_name, format=\"%B%Y\").to_period(\"M\")  # YYYY-MM\n",
        "\n",
        "        print(f\"Processing {file} as {month_str}\")\n",
        "        sat_data = parse_satellite_file(os.path.join(sat_dir, file), month_str)\n",
        "        all_sat.append(sat_data)\n",
        "\n",
        "satellite_all = pd.concat(all_sat, ignore_index=True)\n",
        "\n",
        "# --------------------\n",
        "# STEP 5: Merge CPCB monthly averages with satellite\n",
        "# --------------------\n",
        "final = pd.merge(\n",
        "    cpcb_monthly,\n",
        "    satellite_all,\n",
        "    left_on=[\"Station Name\",\"month\"],\n",
        "    right_on=[\"STATION NAME\",\"month\"],\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "final.to_csv(\"/content/drive/MyDrive/DELHI/merged_NO2_station_satellite.csv\", index=False)\n",
        "print(\"‚úÖ Final merged dataset saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import ast\n",
        "import os\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# --------------------\n",
        "# STEP 1: Load CPCB daily groundtruth\n",
        "# --------------------\n",
        "cpcb = pd.read_csv(\"/content/drive/MyDrive/DELHI/cpcb_daily.csv\")\n",
        "cpcb[\"date\"] = pd.to_datetime(cpcb[\"date\"])\n",
        "cpcb[\"month\"] = cpcb[\"date\"].dt.to_period(\"M\")  # YYYY-MM\n",
        "cpcb_monthly = cpcb.groupby([\"Station Name\",\"month\"])[\"NO2\"].mean().reset_index()\n",
        "\n",
        "# --------------------\n",
        "# STEP 2: Load station locations\n",
        "# --------------------\n",
        "stations = pd.read_csv(\"/content/drive/MyDrive/DELHI/delhi_station_population_2024.csv\")\n",
        "gdf_stations = gpd.GeoDataFrame(\n",
        "    stations,\n",
        "    geometry=gpd.points_from_xy(stations[\"Longitude\"], stations[\"Latitude\"]),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "# --------------------\n",
        "# STEP 3: Function to parse one satellite file\n",
        "# --------------------\n",
        "def parse_satellite_file(file_path, month_str):\n",
        "    sat = pd.read_csv(file_path)\n",
        "\n",
        "    # Parse lat/lon from geo column\n",
        "    def parse_coords(geo_str):\n",
        "        try:\n",
        "            geo_dict = ast.literal_eval(geo_str)\n",
        "            return geo_dict[\"coordinates\"][0], geo_dict[\"coordinates\"][1]\n",
        "        except:\n",
        "            return None, None\n",
        "\n",
        "    sat[[\"lon\", \"lat\"]] = sat[\".geo\"].apply(lambda g: pd.Series(parse_coords(g)))\n",
        "\n",
        "    gdf_sat = gpd.GeoDataFrame(\n",
        "        sat,\n",
        "        geometry=gpd.points_from_xy(sat[\"lon\"], sat[\"lat\"]),\n",
        "        crs=\"EPSG:4326\"\n",
        "    )\n",
        "\n",
        "    # nearest join\n",
        "    joined = gpd.sjoin_nearest(gdf_stations, gdf_sat, how=\"left\")\n",
        "\n",
        "    # assign month\n",
        "    joined[\"month\"] = month_str\n",
        "\n",
        "    return joined[[\"STATION NAME\", \"month\", \"tropospheric_NO2_column_number_density\"]]\n",
        "\n",
        "# --------------------\n",
        "# STEP 4: Loop over all satellite monthly files\n",
        "# --------------------\n",
        "sat_dir = \"/content/drive/MyDrive/DELHI/Satellite\"\n",
        "all_sat = []\n",
        "\n",
        "for file in os.listdir(sat_dir):\n",
        "    if file.endswith(\".csv\"):\n",
        "        # Extract month from filename\n",
        "        # Example: \"TROPOMI_NO2_Delhi_April2024.csv\"\n",
        "        month_name = file.split(\"_\")[-1].replace(\".csv\",\"\")  # e.g. \"April2024\"\n",
        "        month_str = pd.to_datetime(month_name, format=\"%B%Y\").to_period(\"M\")  # YYYY-MM\n",
        "\n",
        "        print(f\"Processing {file} as {month_str}\")\n",
        "        sat_data = parse_satellite_file(os.path.join(sat_dir, file), month_str)\n",
        "        all_sat.append(sat_data)\n",
        "\n",
        "satellite_all = pd.concat(all_sat, ignore_index=True)\n",
        "\n",
        "# --------------------\n",
        "# STEP 5: Merge CPCB monthly averages with satellite\n",
        "# --------------------\n",
        "final = pd.merge(\n",
        "    cpcb_monthly,\n",
        "    satellite_all,\n",
        "    left_on=[\"Station Name\",\"month\"],\n",
        "    right_on=[\"STATION NAME\",\"month\"],\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "final.to_csv(\"/content/drive/MyDrive/DELHI/merged_NO2_station_satellite.csv\", index=False)\n",
        "print(\"‚úÖ Final merged dataset saved!\")\n"
      ],
      "metadata": {
        "id": "p1wD9qGMzN5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --------------------\n",
        "# STEP 1: Load CPCB groundtruth\n",
        "# --------------------\n",
        "cpcb = pd.read_csv(\"/content/drive/MyDrive/DELHI/cpcb_daily.csv\")\n",
        "cpcb[\"date\"] = pd.to_datetime(cpcb[\"date\"])\n",
        "\n",
        "# --------------------\n",
        "# STEP 2: Load Satellite daily dataset\n",
        "# --------------------\n",
        "sat = pd.read_csv(\"/content/drive/MyDrive/DELHI/Delhi_NO2_Daily_Satellite_2024.csv\")\n",
        "sat[\"date\"] = pd.to_datetime(sat[\"date\"])\n",
        "\n",
        "# --------------------\n",
        "# STEP 3: Merge on date\n",
        "# --------------------\n",
        "# Rename for clarity\n",
        "sat = sat.rename(columns={\"NO2\": \"Satellite_NO2\"})\n",
        "\n",
        "# Merge: each station‚Äôs daily CPCB NO2 with same day‚Äôs satellite mean NO2\n",
        "merged = pd.merge(\n",
        "    cpcb,\n",
        "    sat[[\"date\", \"Satellite_NO2\"]],\n",
        "    on=\"date\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "# --------------------\n",
        "# STEP 4: Save\n",
        "# --------------------\n",
        "merged.to_csv(\"/content/drive/MyDrive/DELHI/merged_NO2_daily.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Final merged daily dataset saved!\")\n",
        "print(merged.head())\n"
      ],
      "metadata": {
        "id": "i4XcQ_CKz-IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --------------------\n",
        "# STEP 1: Load merged daily NO2 dataset\n",
        "# --------------------\n",
        "merged = pd.read_csv(\"/content/drive/MyDrive/DELHI/merged_NO2_daily.csv\")\n",
        "\n",
        "# --------------------\n",
        "# STEP 2: Load population and nightlights datasets\n",
        "# --------------------\n",
        "pop = pd.read_csv(\"/content/drive/MyDrive/DELHI/delhi_station_population_2024.csv\")\n",
        "lights = pd.read_csv(\"/content/drive/MyDrive/DELHI/delhi_stations_ntl_2024_average.csv\")\n",
        "\n",
        "# Ensure consistent station name column\n",
        "for df in [pop, lights]:\n",
        "    if \"Station Name\" not in df.columns:\n",
        "        # Rename the 'name' column to 'Station Name' in the lights dataframe\n",
        "        if \"name\" in df.columns:\n",
        "             df.rename(columns={\"name\": \"Station Name\"}, inplace=True)\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# STEP 3: Merge all datasets\n",
        "# --------------------\n",
        "final = merged.merge(pop[[\"Station Name\", \"population_2024\"]], on=\"Station Name\", how=\"left\")\n",
        "# Use 'avg_rad' column for nightlights data\n",
        "final = final.merge(lights[[\"Station Name\", \"avg_rad\"]], on=\"Station Name\", how=\"left\")\n",
        "final.rename(columns={\"avg_rad\": \"nightlights_2024\"}, inplace=True)\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# STEP 4: Save\n",
        "# --------------------\n",
        "final.to_csv(\"/content/drive/MyDrive/DELHI/merged_NO2_population_nightlights.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Final dataset with Population + Nighttime Lights saved!\")\n",
        "print(final.head())"
      ],
      "metadata": {
        "id": "inm4kb-h55m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --------------------\n",
        "# STEP 1: Load merged daily NO2 dataset\n",
        "# --------------------\n",
        "merged = pd.read_csv(\"/content/drive/MyDrive/DELHI/merged_NO2_daily.csv\")\n",
        "\n",
        "# --------------------\n",
        "# STEP 2: Load population and nightlights datasets\n",
        "# --------------------\n",
        "pop = pd.read_csv(\"/content/drive/MyDrive/DELHI/delhi_station_population_2024.csv\")\n",
        "lights = pd.read_csv(\"/content/drive/MyDrive/DELHI/delhi_stations_ntl_2024_average _2.csv\")\n",
        "\n",
        "# Ensure consistent station name column\n",
        "for df in [pop, lights]:\n",
        "    if \"Station Name\" not in df.columns:\n",
        "        df.rename(columns={col: \"Station Name\" for col in df.columns if \"Station\" in col}, inplace=True)\n",
        "\n",
        "# --------------------\n",
        "# STEP 3: Merge all datasets\n",
        "# --------------------\n",
        "final = merged.merge(pop[[\"Station Name\", \"population_2024\"]], on=\"Station Name\", how=\"left\")\n",
        "final = final.merge(lights[[\"Station Name\", \"avg_rad\"]], on=\"Station Name\", how=\"left\")\n",
        "\n",
        "# --------------------\n",
        "# STEP 4: Save\n",
        "# --------------------\n",
        "final.to_csv(\"/content/drive/MyDrive/DELHI/merged_NO2_population_nightlights.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Final dataset with Population + Nighttime Lights saved!\")\n",
        "print(final.head())\n"
      ],
      "metadata": {
        "id": "wZJ4qsiT7saE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# --------------------\n",
        "# STEP 1: Mount Drive\n",
        "# --------------------\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --------------------\n",
        "# STEP 2: File paths\n",
        "# --------------------\n",
        "groundtruth_file = \"/content/drive/MyDrive/DELHI/merged_NO2_daily.csv\"  # your NO2+satellite merged file\n",
        "population_file  = \"/content/drive/MyDrive/DELHI/delhi_station_population_2024.csv\"\n",
        "night_file       = \"/content/drive/MyDrive/DELHI/delhi_stations_ntl_2024_average _2.csv\"\n",
        "\n",
        "output_file      = \"/content/drive/MyDrive/DELHI/final_merged_dataset.csv\"\n",
        "\n",
        "# --------------------\n",
        "# STEP 3: Load datasets\n",
        "# --------------------\n",
        "df_no2   = pd.read_csv(groundtruth_file)\n",
        "df_pop   = pd.read_csv(population_file)\n",
        "df_night = pd.read_csv(night_file)\n",
        "\n",
        "# --------------------\n",
        "# STEP 4: Clean station names\n",
        "# --------------------\n",
        "def clean_station_names(df, col=\"Station Name\"):\n",
        "    \"\"\"\n",
        "    Cleans station names by:\n",
        "    - Removing anything in parentheses e.g. \"Alipur (1)\" -> \"Alipur\"\n",
        "    - Stripping extra spaces\n",
        "    - Converting to uppercase (so merge is case-insensitive)\n",
        "    \"\"\"\n",
        "    df[col] = df[col].astype(str) \\\n",
        "                     .apply(lambda x: re.sub(r\"\\s*\\(.*\\)\", \"\", x)) \\\n",
        "                     .str.strip() \\\n",
        "                     .str.upper()\n",
        "    return df\n",
        "\n",
        "df_no2   = clean_station_names(df_no2, \"Station Name\")\n",
        "df_pop   = clean_station_names(df_pop, \"STATION NAME\") # Pass the correct column name for df_pop\n",
        "df_night = clean_station_names(df_night, \"Station Name\")\n",
        "\n",
        "# --------------------\n",
        "# STEP 5: Merge datasets\n",
        "# --------------------\n",
        "# First merge NO2+satellite with population\n",
        "df_merged = df_no2.merge(df_pop, left_on=\"Station Name\", right_on=\"STATION NAME\", how=\"left\") # Use left_on and right_on for merging\n",
        "df_merged = df_merged.drop(columns=\"STATION NAME\") # Drop the extra column from the population dataframe\n",
        "\n",
        "# Then merge with nighttime dataset\n",
        "df_merged = df_merged.merge(df_night, on=\"Station Name\", how=\"left\")\n",
        "\n",
        "# --------------------\n",
        "# STEP 6: Save final dataset\n",
        "# --------------------\n",
        "df_merged.to_csv(output_file, index=False)\n",
        "print(f\"‚úÖ Final merged dataset saved at: {output_file}\")\n",
        "print(df_merged.head())"
      ],
      "metadata": {
        "id": "LUBAgwvQcjpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# --------------------\n",
        "# STEP 1: Mount Drive\n",
        "# --------------------\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --------------------\n",
        "# STEP 2: File paths\n",
        "# --------------------\n",
        "groundtruth_file = \"/content/drive/MyDrive/DELHI/merged_NO2_daily.csv\"  # NO2+satellite merged file\n",
        "population_file  = \"/content/drive/MyDrive/DELHI/delhi_station_population_2024_2.csv\"\n",
        "night_file       = \"/content/drive/MyDrive/DELHI/delhi_stations_ntl_2024_average _2.csv\"\n",
        "\n",
        "output_file      = \"/content/drive/MyDrive/DELHI/final_merged_dataset_cleaned.csv\"\n",
        "\n",
        "# --------------------\n",
        "# STEP 3: Load datasets\n",
        "# --------------------\n",
        "df_no2   = pd.read_csv(groundtruth_file)\n",
        "df_pop   = pd.read_csv(population_file)\n",
        "df_night = pd.read_csv(night_file)\n",
        "\n",
        "# --------------------\n",
        "# STEP 4: Clean station names (make consistent)\n",
        "# --------------------\n",
        "def clean_station_names(df, col=\"Station Name\"):\n",
        "    \"\"\"\n",
        "    Cleans station names:\n",
        "    - Removes text in parentheses: 'Alipur (1)' -> 'Alipur'\n",
        "    - Strips spaces\n",
        "    - Converts to Title Case (First letter capital, rest lower) for consistency\n",
        "    \"\"\"\n",
        "    df[col] = df[col].astype(str) \\\n",
        "                     .apply(lambda x: re.sub(r\"\\s*\\(.*\\)\", \"\", x)) \\\n",
        "                     .str.strip() \\\n",
        "                     .str.title()\n",
        "    return df\n",
        "\n",
        "df_no2   = clean_station_names(df_no2, \"Station Name\")\n",
        "df_pop   = clean_station_names(df_pop, \"Station Name\")\n",
        "df_night = clean_station_names(df_night, \"Station Name\")\n",
        "\n",
        "# --------------------\n",
        "# STEP 5: Drop unnecessary columns\n",
        "# --------------------\n",
        "drop_cols = [\"Sr No\", \"system:index\", \".geo\", \"Longitude\", \"Latitude\"]  # adjust if present\n",
        "for col in drop_cols:\n",
        "    if col in df_no2.columns:\n",
        "        df_no2 = df_no2.drop(columns=[col])\n",
        "    if col in df_pop.columns:\n",
        "        df_pop = df_pop.drop(columns=[col])\n",
        "    if col in df_night.columns:\n",
        "        df_night = df_night.drop(columns=[col])\n",
        "\n",
        "# --------------------\n",
        "# STEP 6: Merge datasets\n",
        "# --------------------\n",
        "df_merged = df_no2.merge(df_pop, on=\"Station Name\", how=\"left\") # Corrected merge key\n",
        "df_merged = df_merged.merge(df_night, on=\"Station Name\", how=\"left\")\n",
        "\n",
        "# --------------------\n",
        "# STEP 7: Save cleaned dataset\n",
        "# --------------------\n",
        "df_merged.to_csv(output_file, index=False)\n",
        "print(f\"‚úÖ Cleaned dataset saved at: {output_file}\")\n",
        "print(\"Preview:\")\n",
        "print(df_merged.head(20))"
      ],
      "metadata": {
        "id": "aNcPHCtUgayJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# --------------------\n",
        "# STEP 1: Mount Drive\n",
        "# --------------------\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --------------------\n",
        "# STEP 2: File paths\n",
        "# --------------------\n",
        "groundtruth_file = \"/content/drive/MyDrive/DELHI/merged_NO2_daily.csv\"  # NO2+satellite merged file\n",
        "population_file  = \"/content/drive/MyDrive/DELHI/delhi_station_population_2024_2.csv\"\n",
        "night_file       = \"/content/drive/MyDrive/DELHI/delhi_stations_ntl_2024_average _2.csv\"\n",
        "\n",
        "output_file      = \"/content/drive/MyDrive/DELHI/final_merged_dataset_cleaned_2.csv\"\n",
        "\n",
        "# --------------------\n",
        "# STEP 3: Load datasets\n",
        "# --------------------\n",
        "df_no2   = pd.read_csv(groundtruth_file)\n",
        "df_pop   = pd.read_csv(population_file)\n",
        "df_night = pd.read_csv(night_file)\n",
        "\n",
        "# --------------------\n",
        "# STEP 4: Clean station names\n",
        "# --------------------\n",
        "def clean_station_names(df, col=\"Station Name\"):\n",
        "    \"\"\"\n",
        "    Cleans station names:\n",
        "    - Removes parentheses like 'Alipur (1)' -> 'Alipur'\n",
        "    - Strips spaces\n",
        "    - Converts to Title Case (Alipur, Wazirpur, etc.)\n",
        "    \"\"\"\n",
        "    df[col] = df[col].astype(str) \\\n",
        "                     .apply(lambda x: re.sub(r\"\\s*\\(.*\\)\", \"\", x)) \\\n",
        "                     .str.strip() \\\n",
        "                     .str.title()\n",
        "    return df\n",
        "\n",
        "df_no2   = clean_station_names(df_no2, \"Station Name\")\n",
        "df_pop   = clean_station_names(df_pop, \"Station Name\")\n",
        "df_night = clean_station_names(df_night, \"Station Name\")\n",
        "\n",
        "# --------------------\n",
        "# STEP 5: Drop unnecessary columns\n",
        "# --------------------\n",
        "drop_cols = [\"SR NO.\", \"Station category\", \"system:index\", \".geo\"]\n",
        "for df in [df_no2, df_pop, df_night]:\n",
        "    for col in drop_cols:\n",
        "        if col in df.columns:\n",
        "            df.drop(columns=[col], inplace=True)\n",
        "\n",
        "# --------------------\n",
        "# STEP 6: Check station names before merge\n",
        "# --------------------\n",
        "print(\"‚úÖ Groundtruth stations:\", sorted(df_no2[\"Station Name\"].unique()))\n",
        "print(\"‚úÖ Population stations:\", sorted(df_pop[\"Station Name\"].unique()))\n",
        "print(\"‚úÖ Nighttime stations:\", sorted(df_night[\"Station Name\"].unique()))\n",
        "\n",
        "# --------------------\n",
        "# STEP 7: Merge datasets\n",
        "# --------------------\n",
        "df_merged = df_no2.merge(df_pop, on=\"Station Name\", how=\"left\")\n",
        "df_merged = df_merged.merge(df_night, on=\"Station Name\", how=\"left\")\n",
        "\n",
        "# --------------------\n",
        "# STEP 8: Save merged dataset\n",
        "# --------------------\n",
        "df_merged.to_csv(output_file, index=False)\n",
        "print(f\"‚úÖ Cleaned & merged dataset saved at: {output_file}\")\n",
        "print(\"Preview:\")\n",
        "print(df_merged.head(20))\n"
      ],
      "metadata": {
        "id": "MttCM013kFXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# ----------------------\n",
        "# STEP 1: Mount Drive\n",
        "# ----------------------\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ----------------------\n",
        "# STEP 2: Base folder\n",
        "# ----------------------\n",
        "base_dir = \"/content/drive/MyDrive/DELHI/GroundTruth_Data\"  # üîÅ change if needed\n",
        "\n",
        "# ----------------------\n",
        "# STEP 3: Merge station data\n",
        "# ----------------------\n",
        "all_stations = []\n",
        "\n",
        "for station_folder in os.listdir(base_dir):\n",
        "    station_path = os.path.join(base_dir, station_folder)\n",
        "\n",
        "    if os.path.isdir(station_path):  # Only enter station directories\n",
        "        print(f\"üìÇ Processing station: {station_folder}\")\n",
        "        monthly_data = []\n",
        "\n",
        "        for file in os.listdir(station_path):\n",
        "            if file.endswith(\".csv\") or file.endswith(\".xlsx\"):\n",
        "                file_path = os.path.join(station_path, file)\n",
        "\n",
        "                try:\n",
        "                    # Read CSV/Excel\n",
        "                    if file.endswith(\".csv\"):\n",
        "                        df = pd.read_csv(file_path, skiprows=16)  # skip headers if needed\n",
        "                    else:\n",
        "                        df = pd.read_excel(file_path, skiprows=16)\n",
        "\n",
        "                    # Ensure correct columns exist\n",
        "                    if \"From Date\" in df.columns and \"NO2\" in df.columns:\n",
        "                        df[\"date\"] = pd.to_datetime(df[\"From Date\"], errors=\"coerce\")\n",
        "                        df[\"NO2\"] = pd.to_numeric(df[\"NO2\"], errors=\"coerce\")\n",
        "                        df[\"Station Name\"] = station_folder\n",
        "                        monthly_data.append(df[[\"Station Name\", \"date\", \"NO2\"]])\n",
        "                    else:\n",
        "                        print(f\"‚ö†Ô∏è Skipping {file} - missing columns\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error reading {file}: {e}\")\n",
        "\n",
        "        # Merge all 12 months for this station\n",
        "        if monthly_data:\n",
        "            station_df = pd.concat(monthly_data, ignore_index=True)\n",
        "            all_stations.append(station_df)\n",
        "\n",
        "# ----------------------\n",
        "# STEP 4: Merge all 39 stations\n",
        "# ----------------------\n",
        "cpcb_all = pd.concat(all_stations, ignore_index=True)\n",
        "\n",
        "# ----------------------\n",
        "# STEP 5: Clean & save\n",
        "# ----------------------\n",
        "cpcb_all.dropna(subset=[\"date\", \"NO2\"], inplace=True)  # remove invalid rows\n",
        "cpcb_all.sort_values([\"Station Name\", \"date\"], inplace=True)\n",
        "\n",
        "output_file = \"/content/drive/MyDrive/DELHI/cpcb_groundtruth_2024.csv\"\n",
        "cpcb_all.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"‚úÖ Final CPCB dataset saved at: {output_file}\")\n",
        "print(cpcb_all.head(20))\n"
      ],
      "metadata": {
        "id": "EKJknt7LuEhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os#IMPORTANT SNIPPET FOR MERGING DATASET\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# ----------------------\n",
        "# STEP 1: Mount Drive\n",
        "# ----------------------\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ---- CONFIG ----\n",
        "station_folder = \"/content/drive/MyDrive/DELHI/GroundTruth_Data/Rohini\"\n",
        "\n",
        "# Columns in your Excel/CSV files\n",
        "from_col = \"From Date\"\n",
        "to_col   = \"To Date\"\n",
        "no2_col  = \"NO2\"\n",
        "\n",
        "# ---- STEP 1: Collect all monthly files ----\n",
        "all_files = [os.path.join(station_folder, f)\n",
        "             for f in os.listdir(station_folder)\n",
        "             if f.endswith(\".csv\") or f.endswith(\".xlsx\")]\n",
        "\n",
        "dfs = []\n",
        "for f in all_files:\n",
        "    try:\n",
        "        if f.endswith(\".csv\"):\n",
        "            # First 16 rows are junk ‚Üí keep row 17 as header\n",
        "            df = pd.read_csv(f, skiprows=15)\n",
        "        else:\n",
        "            df = pd.read_excel(f, skiprows=15)\n",
        "\n",
        "        # Ensure dataframe has at least 3 columns before renaming\n",
        "        if df.shape[1] >= 3:\n",
        "            # Ensure column renaming is correct\n",
        "            df.rename(columns={df.columns[0]: \"From Date\",\n",
        "                               df.columns[1]: \"To Date\",\n",
        "                               df.columns[2]: \"NO2\"}, inplace=True)\n",
        "            dfs.append(df)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Skipping file {os.path.basename(f)}: Does not have enough columns.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading file {os.path.basename(f)}: {e}\")\n",
        "\n",
        "\n",
        "# Merge all months together\n",
        "if dfs: # Check if dfs is not empty\n",
        "    df_station = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # ---- STEP 2: Parse datetime ----\n",
        "    df_station[from_col] = pd.to_datetime(df_station[from_col], errors=\"coerce\")\n",
        "\n",
        "    # Drop rows where timestamp is missing\n",
        "    df_station = df_station.dropna(subset=[from_col])\n",
        "\n",
        "    # ---- STEP 3: Daily average ----\n",
        "    df_daily = df_station.groupby(df_station[from_col].dt.date)[no2_col].mean().reset_index()\n",
        "\n",
        "    # Rename columns\n",
        "    df_daily.rename(columns={from_col: \"date\", no2_col: \"NO2_daily\"}, inplace=True)\n",
        "\n",
        "    # ---- STEP 4: Save ----\n",
        "    out_path = os.path.join(station_folder, \"station_daily_merged.csv\")\n",
        "    df_daily.to_csv(out_path, index=False)\n",
        "\n",
        "    print(f\"‚úÖ Done! Daily averaged file saved at: {out_path}\")\n",
        "else:\n",
        "    print(f\"‚ùå No valid files found in {station_folder} to process.\")"
      ],
      "metadata": {
        "id": "ZCos5m2dxBnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os#IMPORTANT SNIPPET FOR MERGING DATASET\n",
        "import pandas as pd\n",
        "\n",
        "# Paths\n",
        "station_folder = \"/content/drive/MyDrive/DELHI/GroundTruth_Data/Rohini\" # Corrected typo in folder name\n",
        "station_meta_file = \"/content/drive/MyDrive/DELHI/Delhi_weatherst_lat_lon (1).csv\"  # <-- your station lat/lon file\n",
        "\n",
        "# Load the daily dataset created earlier\n",
        "df_daily = pd.read_csv(os.path.join(station_folder, \"station_daily_merged.csv\"))\n",
        "\n",
        "# Load station metadata\n",
        "df_meta = pd.read_csv(station_meta_file)\n",
        "\n",
        "# Clean station name for consistency\n",
        "df_meta[\"Station Name\"] = df_meta[\"Station Name\"].str.strip().str.upper()\n",
        "station_name = os.path.basename(station_folder).strip().upper()\n",
        "\n",
        "# Select the correct station metadata row\n",
        "meta_row = df_meta[df_meta[\"Station Name\"] == station_name].iloc[0]\n",
        "\n",
        "# Add station info to the dataframe\n",
        "df_daily.insert(0, \"Station Name\", meta_row[\"Station Name\"])\n",
        "df_daily.insert(1, \"Latitude\", meta_row[\"Latitude\"])\n",
        "df_daily.insert(2, \"Longitude\", meta_row[\"Longitude\"])\n",
        "\n",
        "# Rename NO2 column\n",
        "df_daily.rename(columns={\"NO2_daily\": \"NO2\"}, inplace=True)\n",
        "\n",
        "# Save final enriched dataset\n",
        "out_path = os.path.join(station_folder, \"station_daily_enriched.csv\")\n",
        "df_daily.to_csv(out_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Final enriched dataset saved at: {out_path}\")"
      ],
      "metadata": {
        "id": "MgO_qAkW04MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive#NOT IN ORDER MERGEING\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "# Folder where all 12 CPCB Excel files are stored\n",
        "folder_path = '/content/drive/MyDrive/DELHI/GroundTruth_Data/okhla Phase-2'   # üîÅ update this to your folder path\n",
        "\n",
        "# Path to your station details CSV (contains Station Name, Latitude, Longitude)\n",
        "station_csv = '/content/drive/MyDrive/DELHI/Delhi_weatherst_lat_lon (1).csv'  # üîÅ update path\n",
        "\n",
        "# === LOAD STATION DETAILS ===\n",
        "station_df = pd.read_csv(station_csv)\n",
        "station_df['Station Name'] = station_df['Station Name'].str.strip().str.upper()\n",
        "\n",
        "# === PROCESS ALL EXCEL FILES ===\n",
        "excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))\n",
        "merged_data = []\n",
        "\n",
        "for file in excel_files:\n",
        "    filename = os.path.basename(file)\n",
        "    station_name_guess = filename.split('.')[0].split('_')[0].upper()  # e.g., 'okhlaphase2' ‚Üí 'OKHLAPHASE2'\n",
        "\n",
        "    print(f\"üìÑ Processing: {filename}\")\n",
        "\n",
        "    # Read Excel, skip header rows until data starts (around row 17)\n",
        "    df = pd.read_excel(file, skiprows=16)\n",
        "\n",
        "    # Keep only relevant columns if they exist\n",
        "    if 'From Date' not in df.columns or 'NO2' not in df.columns:\n",
        "        print(f\"‚ö†Ô∏è Skipping {filename} (required columns missing)\")\n",
        "        continue\n",
        "\n",
        "    df = df[['From Date', 'NO2']].dropna()\n",
        "    # Corrected date format to '%d-%m-%Y %H:%M'\n",
        "    df['date'] = pd.to_datetime(df['From Date'], format='%d-%m-%Y %H:%M').dt.date\n",
        "\n",
        "    # Match station name from details file\n",
        "    matched = station_df[station_df['Station Name'].str.contains(station_name_guess, case=False, na=False)]\n",
        "\n",
        "    if not matched.empty:\n",
        "        lat = matched.iloc[0]['Latitude']\n",
        "        lon = matched.iloc[0]['Longitude']\n",
        "        station_name = matched.iloc[0]['Station Name']\n",
        "    else:\n",
        "        lat, lon, station_name = None, None, station_name_guess\n",
        "\n",
        "    df['Station Name'] = station_name\n",
        "    df['Latitude'] = lat\n",
        "    df['Longitude'] = lon\n",
        "    df.rename(columns={'NO2': 'no2'}, inplace=True)\n",
        "\n",
        "    merged_data.append(df[['Station Name', 'Latitude', 'Longitude', 'date', 'no2']])\n",
        "\n",
        "# === MERGE ALL FILES ===\n",
        "# === MERGE ALL FILES ===\n",
        "if merged_data:\n",
        "    final_df = pd.concat(merged_data, ignore_index=True)\n",
        "    output_dir = '/content/drive/MyDrive/CPCB_excels'\n",
        "    os.makedirs(output_dir, exist_ok=True)   # ‚úÖ Create folder if not exists\n",
        "    output_path = os.path.join(output_dir, 'Delhi_NO2_groundtruth_2024.csv')\n",
        "\n",
        "    final_df.to_csv(output_path, index=False)\n",
        "    print(f\"‚úÖ Combined dataset saved to:\\n{output_path}\")\n",
        "else:\n",
        "    print(\"‚ùå No valid Excel files processed.\")\n"
      ],
      "metadata": {
        "id": "zEKoqAzRiB8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive#OKHLA PHASE DATASET MERGE\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "\n",
        "# === MOUNT DRIVE ===\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "folder_path = '/content/drive/MyDrive/DELHI/GroundTruth_Data/okhla Phase-2'  # folder with all monthly Excel files\n",
        "station_csv = '/content/drive/MyDrive/DELHI/Delhi_weatherst_lat_lon (1).csv'  # station details CSV\n",
        "output_dir = '/content/drive/MyDrive/CPCB_excels'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === LOAD STATION DETAILS ===\n",
        "station_df = pd.read_csv(station_csv)\n",
        "station_df['Station Name'] = station_df['Station Name'].str.strip().str.upper()\n",
        "\n",
        "# === MONTH ORDER MAP ===\n",
        "month_order = {\n",
        "    'JAN': 1, 'FEB': 2, 'MAR': 3, 'APR': 4,\n",
        "    'MAY': 5, 'JUN': 6, 'JUL': 7, 'AUG': 8,\n",
        "    'SEP': 9, 'OCT': 10, 'NOV': 11, 'DEC': 12\n",
        "}\n",
        "\n",
        "# === PROCESS EXCEL FILES ===\n",
        "excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))\n",
        "merged_data = []\n",
        "\n",
        "for file in excel_files:\n",
        "    filename = os.path.basename(file)\n",
        "    print(f\"üìÑ Processing: {filename}\")\n",
        "\n",
        "    # Extract month name from filename (e.g., 'jan_okhlaphase2.xlsx' ‚Üí 'JAN')\n",
        "    month_match = re.search(r'(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)', filename, re.IGNORECASE)\n",
        "    month_name = month_match.group(1).upper() if month_match else 'UNKNOWN'\n",
        "\n",
        "    # Read Excel (skip headers before data starts)\n",
        "    df = pd.read_excel(file, skiprows=16)\n",
        "\n",
        "    if 'From Date' not in df.columns or 'NO2' not in df.columns:\n",
        "        print(f\"‚ö†Ô∏è Skipping {filename} ‚Äî missing columns\")\n",
        "        continue\n",
        "\n",
        "    df = df[['From Date', 'NO2']].dropna()\n",
        "    df['date'] = pd.to_datetime(df['From Date'], format='%d-%m-%Y %H:%M', errors='coerce').dt.date\n",
        "    df.dropna(subset=['date'], inplace=True)\n",
        "\n",
        "    # Match station name from details\n",
        "    matched = station_df[station_df['Station Name'].str.contains('OKHLA', case=False, na=False)]\n",
        "    if not matched.empty:\n",
        "        lat, lon, station_name = matched.iloc[0][['Latitude', 'Longitude', 'Station Name']]\n",
        "    else:\n",
        "        lat, lon, station_name = None, None, 'OKHLA PHASE-2'\n",
        "\n",
        "    df['Station Name'] = station_name\n",
        "    df['Latitude'] = lat\n",
        "    df['Longitude'] = lon\n",
        "    df.rename(columns={'NO2': 'no2'}, inplace=True)\n",
        "    df['Month'] = month_name\n",
        "    df['Month_Num'] = month_order.get(month_name[:3].upper(), 99)\n",
        "\n",
        "    merged_data.append(df[['Station Name', 'Latitude', 'Longitude', 'date', 'no2', 'Month', 'Month_Num']])\n",
        "\n",
        "# === MERGE AND SORT ===\n",
        "if merged_data:\n",
        "    final_df = pd.concat(merged_data, ignore_index=True)\n",
        "    final_df.sort_values(by=['Month_Num', 'date'], inplace=True)\n",
        "    final_df.drop(columns='Month_Num', inplace=True)\n",
        "\n",
        "    output_path = os.path.join(output_dir, 'Delhi_NO2_groundtruth_2024.csv')\n",
        "    final_df.to_csv(output_path, index=False)\n",
        "\n",
        "    print(f\"‚úÖ Combined dataset saved to:\\n{output_path}\")\n",
        "    print(f\"‚úÖ Rows: {len(final_df)} | Columns: {list(final_df.columns)}\")\n",
        "else:\n",
        "    print(\"‚ùå No valid Excel files processed.\")\n"
      ],
      "metadata": {
        "id": "g6PR7Jrfn3HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd #reordering(DIDN'T WORKED)\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "\n",
        "# === MOUNT DRIVE ===\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load your merged dataset\n",
        "file_path = '/content/drive/MyDrive/CPCB_excels/Delhi_NO2_groundtruth_2024.csv'  # üîÅ update if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# --- Define correct month order ---\n",
        "month_order = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN',\n",
        "                'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n",
        "\n",
        "# --- Fix typo: \"FAB\" ‚Üí \"FEB\" if any ---\n",
        "df['Month'] = df['Month'].replace({'FAB': 'FEB', 'FEBRUARY': 'FEB'})\n",
        "\n",
        "# --- Create sort key and reorder ---\n",
        "df['Month_Num'] = df['Month'].map({m: i for i, m in enumerate(month_order, 1)})\n",
        "df.sort_values(by=['Month_Num', 'date'], inplace=True)\n",
        "df.drop(columns='Month_Num', inplace=True)\n",
        "\n",
        "# --- Save reordered version ---\n",
        "output_path = '/content/drive/MyDrive/CPCB_excels/Delhi_NO2_groundtruth_2024_sorted.csv'\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Reordered file saved to:\\n{output_path}\")"
      ],
      "metadata": {
        "id": "lNPki_tql06A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive#REORDERING OF OKHLA PHASE\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# === MOUNT DRIVE ===\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load your merged dataset\n",
        "file_path = '/content/drive/MyDrive/CPCB_excels/Delhi_NO2_groundtruth_2024.csv'  # üîÅ update if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# --- Ensure 'date' is a proper datetime ---\n",
        "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "# --- Sort chronologically based on date ---\n",
        "df = df.sort_values(by='date')\n",
        "\n",
        "# --- Optional: Extract month name (for readability) ---\n",
        "df['Month'] = df['date'].dt.strftime('%b').str.upper()  # e.g., JAN, FEB, MAR\n",
        "\n",
        "# --- Save reordered version ---\n",
        "output_path = '/content/drive/MyDrive/CPCB_excels/Delhi_NO2_groundtruth_2024_sorted.csv'\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Reordered chronologically and saved to:\\n{output_path}\")\n"
      ],
      "metadata": {
        "id": "rbFSVyyjom4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# 1. Mount Google Drive\n",
        "# =====================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# =====================================\n",
        "# 2. Import libraries\n",
        "# =====================================\n",
        "import pandas as pd\n",
        "\n",
        "# =====================================\n",
        "# 3. Load datasets\n",
        "# =====================================\n",
        "# üîÅ Update file paths as per your Drive folder\n",
        "groundtruth_path = '/content/drive/MyDrive/DELHI/groundtruth_no2_okhla.csv'   # Ground truth dataset\n",
        "satellite_path = '/content/drive/MyDrive/no2_data/TROPOMI_NO2_Delhi_Jan2024_clean.csv'  # GEE satellite dataset\n",
        "\n",
        "gt_df = pd.read_csv(groundtruth_path)\n",
        "sat_df = pd.read_csv(satellite_path)\n",
        "\n",
        "# =====================================\n",
        "# 4. Standardize and clean columns\n",
        "# =====================================\n",
        "# Rename columns for consistency\n",
        "gt_df.rename(columns={\n",
        "    'Station Name': 'station',\n",
        "    'Latitude': 'lat',\n",
        "    'Longitude': 'lon',\n",
        "    'date': 'date',\n",
        "    'no2': 'groundtruth_no2'\n",
        "}, inplace=True)\n",
        "\n",
        "sat_df.rename(columns={\n",
        "    'name': 'station',\n",
        "    'tropospheric_NO2_column_number_density': 'tropospheric_no2'\n",
        "}, inplace=True)\n",
        "\n",
        "# Convert date columns to datetime\n",
        "gt_df['date'] = pd.to_datetime(gt_df['date'])\n",
        "sat_df['date'] = pd.to_datetime(sat_df['date'])\n",
        "\n",
        "# =====================================\n",
        "# 5. Merge datasets based on date\n",
        "# =====================================\n",
        "merged = pd.merge(gt_df, sat_df[['date', 'tropospheric_no2']], on='date', how='left')\n",
        "\n",
        "# =====================================\n",
        "# 6. Reorder columns for clarity\n",
        "# =====================================\n",
        "final_df = merged[['station', 'lat', 'lon', 'date', 'groundtruth_no2', 'tropospheric_no2']]\n",
        "\n",
        "# =====================================\n",
        "# 7. Save to Google Drive\n",
        "# =====================================\n",
        "output_path = '/content/drive/MyDrive/no2_data/merged_okhla_no2_2024.csv'\n",
        "final_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Merged CSV saved to:\\n{output_path}\")\n",
        "\n",
        "# Display first few rows\n",
        "final_df.head()\n"
      ],
      "metadata": {
        "id": "-VMvnlrVxH2C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}