{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMPpCPNFW70hH5cBjGMuYY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepti-Shringare/Downscaling_of_no2map_XGBoost/blob/main/COLAB_CODES/Final_feature_Set_merging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***FINAL DATASET MERGING\n",
        "***1."
      ],
      "metadata": {
        "id": "pGlB0ePNCMY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# =========================\n",
        "# 1. Load datasets\n",
        "# =========================\n",
        "no2_path = \"/content/drive/MyDrive/MAJOR_PROJECT/Ground_Satellite_Pop_Nighttime/MAJOR_DHYAND_CHAND_NATIONAL_STADIUM_2024_NO2_with_population_nightlights.csv\"\n",
        "met_path = \"/content/drive/MyDrive/MAJOR_PROJECT/ERA5_LAND_U10_V10_Winddspeed_temp(c)_pressure/ERA5_LAND_Daily_Met_MAJOR DHYAND CHAND NATIONAL STADIUM_2024.csv\"\n",
        "blh_path = \"/content/drive/MyDrive/MAJOR_PROJECT/ERA5_ATMOS/ERA5_ATMOS_DAILY_BLH_TCC_MAJOR DHYAND CHAND NATIONAL STADIUM_2024.csv\"\n",
        "\n",
        "no2_df = pd.read_csv(no2_path)\n",
        "met_df = pd.read_csv(met_path)\n",
        "blh_df = pd.read_csv(blh_path)\n",
        "\n",
        "# FIX: Rename 'station_name' to 'station' in met_df to ensure consistency\n",
        "if 'station_name' in met_df.columns and 'station' not in met_df.columns:\n",
        "    met_df = met_df.rename(columns={'station_name': 'station'})\n",
        "\n",
        "# =========================\n",
        "# 2. Standardize columns\n",
        "# =========================\n",
        "for df_to_process in [no2_df, met_df, blh_df]:\n",
        "    df_to_process['station'] = df_to_process['station'].str.strip().str.upper()\n",
        "    df_to_process['date'] = pd.to_datetime(df_to_process['date'])\n",
        "\n",
        "# Drop unwanted columns\n",
        "met_df = met_df.drop(columns=['system:index', '.geo'], errors='ignore')\n",
        "\n",
        "# =========================\n",
        "# 3. Merge STEP 1: NO2 + Surface Meteorology\n",
        "# =========================\n",
        "df = no2_df.merge(\n",
        "    met_df[['station','date',\n",
        "            'u10','v10','wind_speed',\n",
        "            'temperature_2m_C','surface_pressure_hPa']],\n",
        "    on=['station','date'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 4. Merge STEP 2: Add BLH + Cloud Cover\n",
        "# =========================\n",
        "df = df.merge(\n",
        "    blh_df[['station','date','BLH','total_cloud_cover']],\n",
        "    on=['station','date'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# 5. Final column order\n",
        "# =========================\n",
        "final_cols = [\n",
        "    'station',\n",
        "    'lat',\n",
        "    'lon',\n",
        "    'date',\n",
        "    'groundtruth_no2',\n",
        "    'tropospheric_no2',\n",
        "    'population_density',\n",
        "    'night_light',\n",
        "    'u10',\n",
        "    'v10',\n",
        "    'wind_speed',\n",
        "    'temperature_2m_C',\n",
        "    'surface_pressure_hPa',\n",
        "    'BLH',\n",
        "    'total_cloud_cover'\n",
        "]\n",
        "\n",
        "df = df[final_cols]\n",
        "\n",
        "# =========================\n",
        "# 6. Save final dataset\n",
        "# =========================\n",
        "output_path = \"/content/drive/MyDrive/MAJOR_PROJECT/Final_ML_features/MAJOR DHYAND CHAND NATIONAL STADIUM_FINAL_ML_FEATURES.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"✅ FINAL ML DATASET SAVED:\")\n",
        "print(output_path)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "0gUJVMJKCb2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os # Import os module for directory listing\n",
        "\n",
        "# ================================\n",
        "# 1. Folder containing station CSVs\n",
        "# ================================\n",
        "input_path = \"/content/drive/MyDrive/MAJOR_PROJECT/Final_ML_features/*.csv\"\n",
        "output_path = \"/content/drive/MyDrive/MAJOR_PROJECT/Final_ML_features/MASTER_NO2_DATASET_2024.csv\"\n",
        "\n",
        "# ================================\n",
        "# 2. Required columns (FINAL ORDER)\n",
        "# ================================\n",
        "final_columns = [\n",
        "    \"station\",\n",
        "    \"lat\",\n",
        "    \"lon\",\n",
        "    \"date\",\n",
        "    \"groundtruth_no2\",\n",
        "    \"tropospheric_no2\",\n",
        "    \"population_density\",\n",
        "    \"night_light\",\n",
        "    \"u10\",\n",
        "    \"v10\",\n",
        "    \"wind_speed\",\n",
        "    \"temperature_2m_C\",\n",
        "    \"surface_pressure_hPa\",\n",
        "    \"BLH\",\n",
        "    \"total_cloud_cover\"\n",
        "]\n",
        "\n",
        "# ================================\n",
        "# 3. Read and merge files\n",
        "# ================================\n",
        "\n",
        "# Diagnostic: Check directory contents before glob\n",
        "folder_to_check = os.path.dirname(input_path.replace('*', ''))\n",
        "print(f\"Checking contents of directory: {folder_to_check}\")\n",
        "if os.path.exists(folder_to_check):\n",
        "    print(f\"Directory contents: {os.listdir(folder_to_check)}\")\n",
        "else:\n",
        "    print(f\"Directory does not exist: {folder_to_check}\")\n",
        "\n",
        "files = glob.glob(input_path)\n",
        "print(f\"Files found by glob: {len(files)}\")\n",
        "\n",
        "df_list = []\n",
        "\n",
        "if not files:\n",
        "    print(\"❌ No CSV files found to merge. Please ensure the previous step generated the files correctly and they are visible in the file system.\")\n",
        "else:\n",
        "    for file in files:\n",
        "        df = pd.read_csv(file)\n",
        "\n",
        "        # Rename common inconsistencies\n",
        "        df = df.rename(columns={\n",
        "            \"station_name\": \"station\",\n",
        "            \"latitude\": \"lat\",\n",
        "            \"longitude\": \"lon\",\n",
        "            \"surface_pressure_hp\": \"surface_pressure_hPa\"\n",
        "        })\n",
        "\n",
        "        # Keep only required columns\n",
        "        df = df[final_columns]\n",
        "\n",
        "        df_list.append(df)\n",
        "\n",
        "    # ================================\n",
        "    # 4. Combine into master dataset\n",
        "    # ================================\n",
        "    master_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "    # Convert date & sort\n",
        "    master_df[\"date\"] = pd.to_datetime(master_df[\"date\"])\n",
        "    master_df = master_df.sort_values([\"station\", \"date\"])\n",
        "\n",
        "    # ================================\n",
        "    # 5. Save output\n",
        "    # ================================\n",
        "    master_df.to_csv(output_path, index=False)\n",
        "\n",
        "    print(\"✅ MERGE COMPLETE\")\n",
        "    print(\"Final shape:\", master_df.shape)\n",
        "    print(\"Stations:\", master_df[\"station\"].nunique())\n"
      ],
      "metadata": {
        "id": "zQLQTCuY3Rqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# ==============================\n",
        "# 1. Path to your folder\n",
        "# ==============================\n",
        "folder_path = \"/content/drive/MyDrive/MAJOR_PROJECT/Final_ML_features\"\n",
        "\n",
        "# ==============================\n",
        "# 2. Read all CSV files\n",
        "# ==============================\n",
        "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
        "\n",
        "print(\"Total files found:\", len(csv_files))\n",
        "\n",
        "df_list = []\n",
        "\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Ensure date is datetime\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    df_list.append(df)\n",
        "\n",
        "# ==============================\n",
        "# 3. Vertical merge (row-wise)\n",
        "# ==============================\n",
        "master_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# ==============================\n",
        "# 4. Sort properly\n",
        "# ==============================\n",
        "master_df = master_df.sort_values(\n",
        "    by=[\"station\", \"date\"]\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# ==============================\n",
        "# 5. Save master dataset\n",
        "# ==============================\n",
        "output_path = \"/content/drive/MyDrive/MAJOR_PROJECT/MASTER_NO2_DATASET_2024.csv\"\n",
        "master_df.to_csv(output_path, index=False)\n",
        "\n",
        "# ==============================\n",
        "# 6. Quick sanity check\n",
        "# ==============================\n",
        "print(\"Merged dataset shape:\", master_df.shape)\n",
        "print(\"Unique stations:\", master_df['station'].nunique())\n",
        "print(\"Date range:\", master_df['date'].min(), \"to\", master_df['date'].max())\n"
      ],
      "metadata": {
        "id": "H3Jne7vx5feY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# -----------------------------\n",
        "# 1. File paths (update if needed)\n",
        "# -----------------------------\n",
        "file_1 = \"/content/drive/MyDrive/DELHI_2024_FINAL_ML_FEATURES_MERGED37.csv\"\n",
        "file_2 = \"/content/drive/MyDrive/MAJOR_PROJECT/Final_ML_features/MAJOR DHYAND CHAND NATIONAL STADIUM_FINAL_ML_FEATURES.csv\"\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Read CSV files\n",
        "# -----------------------------\n",
        "df1 = pd.read_csv(file_1)\n",
        "df2 = pd.read_csv(file_2)\n",
        "\n",
        "print(\"NSIT shape:\", df1.shape)\n",
        "print(\"Mandira Marg shape:\", df2.shape)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Vertical merge (row-wise)\n",
        "# -----------------------------\n",
        "merged_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
        "\n",
        "print(\"Merged shape:\", merged_df.shape)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Save merged file\n",
        "# -----------------------------\n",
        "output_path = \"/content/drive/MyDrive/DELHI_2024_FINAL_ML_FEATURES_MERGED38.csv\"\n",
        "merged_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"✅ Merged file saved at:\", output_path)\n"
      ],
      "metadata": {
        "id": "9yzlKE2K7VAj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}